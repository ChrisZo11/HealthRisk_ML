# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Con9pkp041UA9AuM2sjzfoiQoBoOeYlQ
"""

# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import joblib


# 1. LOAD DATA
df = pd.read_csv("Lifestyle_and_Health_Risk_Prediction_Synthetic_Dataset.csv")

# 2. OUTLIER HANDLING (IQR)
df_clean = df.copy()
numeric_cols = df_clean.select_dtypes(include=["float64", "int64"]).columns

for col in numeric_cols:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df_clean[col] = np.clip(df_clean[col], lower, upper)

# 3. LABEL ENCODING
cols_to_encode = [
    'married', 'exercise', 'sugar_intake',
    'alcohol', 'smoking', 'profession',
    'health_risk'
]

encoders = {}

for col in cols_to_encode:
    le = LabelEncoder()
    df_clean[col] = le.fit_transform(df_clean[col])
    encoders[col] = le


# 4. FEATURE SELECTION
X = df_clean[['age', 'bmi', 'smoking', 'alcohol', 'sleep', 'sugar_intake']]
y = df_clean['health_risk']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# 5. DECISION TREE â€“ HYPERPARAMETER TUNING
dt_param_grid = {
    'max_depth': [3, 5, 7, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

dt_base = DecisionTreeClassifier(random_state=42)

grid_dt = GridSearchCV(
    estimator=dt_base,
    param_grid=dt_param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_dt.fit(X_train, y_train)
best_dt_model = grid_dt.best_estimator_


# 6. KNN â€“ SCALING + HYPERPARAMETER TUNING
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_param_grid = {
    'n_neighbors': range(1, 31, 2),
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

knn_base = KNeighborsClassifier()

grid_knn = GridSearchCV(
    estimator=knn_base,
    param_grid=knn_param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_knn.fit(X_train_scaled, y_train)
best_knn_model = grid_knn.best_estimator_


# 7. SAVE FINAL MODELS + ENCODERS + SCALER
joblib.dump(best_dt_model, "dt_best_model.sav")
joblib.dump(best_knn_model, "knn_best_model.sav")
joblib.dump(scaler, "scaler.sav")
joblib.dump(encoders, "encoders.sav")


# ----------- Evaluate DT -----------
y_pred_dt = best_dt_model.predict(X_test)
dt_acc = accuracy_score(y_test, y_pred_dt)

print("\nAkurasi Decision Tree Terbaik:", dt_acc)
print("\nClassification Report (DT):")
print(classification_report(y_test, y_pred_dt))

cm_dt = confusion_matrix(y_test, y_pred_dt)


# ----------- Evaluate KNN -----------
y_pred_knn = best_knn_model.predict(X_test_scaled)
knn_acc = accuracy_score(y_test, y_pred_knn)

print("\nAkurasi KNN Terbaik:", knn_acc)
print("\nClassification Report (KNN):")
print(classification_report(y_test, y_pred_knn))

cm_knn = confusion_matrix(y_test, y_pred_knn)


# 8. SUMMARY AKURASI MODEL

print("\n==================== RINGKASAN AKURASI ====================")
print(f"Akurasi Decision Tree Terbaik : {dt_acc:.4f}")
print(f"Akurasi KNN Terbaik           : {knn_acc:.4f}")

if dt_acc > knn_acc:
    print("\nğŸ‘‰ Model terbaik adalah: Decision Tree")
else:
    print("\nğŸ‘‰ Model terbaik adalah: KNN")